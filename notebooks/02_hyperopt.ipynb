{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f18c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc9e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df_heart=pd.read_csv('../../statistics-and-machine-learning-learning/data/framingham.csv')\n",
    "#df_heart.replace(np.nan,\"NaN\")\n",
    "df_heart.dropna(axis=0,inplace=True)\n",
    "\n",
    "##separation in X and y\n",
    "X_heart = df_heart.drop( columns = \"TenYearCHD\" )\n",
    "y_heart = df_heart[ \"TenYearCHD\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3db43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's start by splitting the data into a train and a validaiton set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_heart, y_heart, stratify=y_heart, random_state=94)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c577d9",
   "metadata": {},
   "source": [
    "# hyperopt-sklearn\n",
    "\n",
    "We will access hyperopt through the [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn), authored by some of the original hyperopt authors.\n",
    "\n",
    "It offer a convenient interface for classical ML-learning algorithms, with a lot of pre-sets corresponding to sklearn routines and parameters.\n",
    "\n",
    "\n",
    "Unfortunately, the documentation of this library is sparse, \n",
    "and spread between the [github readme](https://github.com/hyperopt/hyperopt-sklearn), the [github page](http://hyperopt.github.io/hyperopt-sklearn/), their [scipy2014 paper](http://conference.scipy.org.s3-website-us-east-1.amazonaws.com/proceedings/scipy2014/pdfs/komer.pdf), and the class and function `help()`.\n",
    "\n",
    "So through a sery of example we will try to demonstrate some of the basic usage of this library, \n",
    "as well as how to do some of the less documented things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887703ee",
   "metadata": {},
   "source": [
    "## basic usage\n",
    "\n",
    "the library implements \"components\" corresponding to sklearn (or sklearn-adjacent) objects with defined hyper-parameter search spaces\n",
    "\n",
    "[list of available components](https://github.com/hyperopt/hyperopt-sklearn?tab=readme-ov-file#available-components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56ed7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing \n",
    "# * HyperoptEstimator -> basic hpsklearn object which wraps the optimization procedure\n",
    "# * svc , standard_scaler -> components for the SVM classifier and standard_scaler\n",
    "\n",
    "from hpsklearn import HyperoptEstimator, svc , standard_scaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77cdf63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.10trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.36trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:21<00:00, 21.79s/trial, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 14.44trial/s, best loss: 0.1657559198542805]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.17trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.36trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.17trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  5.99trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  8.51trial/s, best loss: 0.16393442622950816]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00,  8.93trial/s, best loss: 0.16393442622950816]\n",
      "0.8469945355191257\n",
      "CPU times: user 379 ms, sys: 53 ms, total: 432 ms\n",
      "Wall time: 23.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(classifier=svc(\"mySVC\"), ## the call to svc takes only a name, and will setup a default search space for it hyper-parameters\n",
    "                         trial_timeout=120) ## sometimes the fitting/evaluating process gets stuck\n",
    "## set a timeout to prevent being stuck for too long\n",
    "\n",
    "estim.fit(X_train, y_train)\n",
    "\n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801813d4",
   "metadata": {},
   "source": [
    "Important notes about what happened (ie, the \"hidden\" default parameters):\n",
    "\n",
    " * 80% of the data was used as a train set, 20% as validation set\n",
    " * the 20% validation set are the last elements of the given data\n",
    " * the score being optimized is 1-accuracy(validation set) (hyperopt always tries to minimize)\n",
    " * the optimization procedure ran for a fixed number of 10 rounds\n",
    " * `HyperoptEstimator` has added a random preprocessing step\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daef13cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learner': SVC(C=0.8661828616263555, coef0=0.32310053227283486,\n",
       "     decision_function_shape='ovo', degree=5, random_state=np.int64(3),\n",
       "     shrinking=False, tol=0.0019357643730025579),\n",
       " 'preprocs': (StandardScaler(with_mean=False),),\n",
       " 'ex_preprocs': ()}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## getting the best model\n",
    "estim.best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d0a800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rbf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the chosen kernel, is unfortunately, not shown in the summary above\n",
    "## we can fetch it from the object itself\n",
    "estim.best_model()['learner'].kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61540db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>66.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>27.30</td>\n",
       "      <td>85.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>23.37</td>\n",
       "      <td>58.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>309.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>26.91</td>\n",
       "      <td>70.0</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>23.24</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "1441     0   45        2.0              0         0.0     0.0   \n",
       "4210     1   50        1.0              0         0.0     0.0   \n",
       "1732     0   52        2.0              0         0.0     0.0   \n",
       "2503     1   43        1.0              1        20.0     0.0   \n",
       "3222     0   44        1.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "1441                0             0         0    262.0  116.0   66.0  21.56   \n",
       "4210                0             0         0    282.0  126.5   88.0  27.30   \n",
       "1732                0             0         0    221.0  124.0   69.0  23.37   \n",
       "2503                0             0         1    309.0  124.0   85.0  26.91   \n",
       "3222                0             0         0    200.0  128.0   82.0  23.24   \n",
       "\n",
       "      heartRate  glucose  \n",
       "1441       66.0     76.0  \n",
       "4210       85.0     87.0  \n",
       "1732       58.0     81.0  \n",
       "2503       70.0    215.0  \n",
       "3222       80.0     73.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d049a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predicting on new data using the best model\n",
    "estim.predict( X_test.iloc[:5,:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de5d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.1657559198542805,\n",
       "   'loss_variance': 0.00025233739942982086,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.08312249183654785},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(0)],\n",
       "    'mySVC.svc_coef0': [np.int64(0)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(0)],\n",
       "    'mySVC.svc_gamma': [np.int64(0)],\n",
       "    'mySVC.svc_kernel': [np.int64(0)],\n",
       "    'mySVC.svc_random_state': [np.int64(0)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.int64(0)],\n",
       "    'preprocessing': [np.int64(0)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [np.int64(0)],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(1.0934211895658286)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.792540555539598)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(2)],\n",
       "    'mySVC.svc_gamma': [np.int64(0)],\n",
       "    'mySVC.svc_kernel': [np.int64(1)],\n",
       "    'mySVC.svc_random_state': [np.int64(3)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(0.002969556984771239)],\n",
       "    'preprocessing': [np.int64(3)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [np.int64(2)],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 22, 859000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 22, 956000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.1657559198542805,\n",
       "   'loss_variance': 0.00025233739942982086,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.14423751831054688},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(1)],\n",
       "    'mySVC.svc_coef0': [np.int64(1)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(1)],\n",
       "    'mySVC.svc_degree': [np.int64(1)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(1)],\n",
       "    'mySVC.svc_random_state': [np.int64(1)],\n",
       "    'mySVC.svc_shrinking': [np.int64(1)],\n",
       "    'mySVC.svc_tol': [np.int64(1)],\n",
       "    'preprocessing': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [np.int64(1)],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(1.2775781542665854)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.7456018216312651)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(1)],\n",
       "    'mySVC.svc_degree': [np.int64(4)],\n",
       "    'mySVC.svc_gamma': [np.int64(0)],\n",
       "    'mySVC.svc_kernel': [np.int64(1)],\n",
       "    'mySVC.svc_random_state': [np.int64(2)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(1.249411875400459e-05)],\n",
       "    'preprocessing': [np.int64(3)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [np.int64(2)],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 22, 966000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 23, 121000)},\n",
       " {'state': 2,\n",
       "  'tid': 2,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.1657559198542805,\n",
       "   'loss_variance': 0.00025233739942982086,\n",
       "   'status': 'ok',\n",
       "   'duration': 21.782816886901855},\n",
       "  'misc': {'tid': 2,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(2)],\n",
       "    'mySVC.svc_coef0': [np.int64(2)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(2)],\n",
       "    'mySVC.svc_degree': [np.int64(2)],\n",
       "    'mySVC.svc_gamma': [np.int64(2)],\n",
       "    'mySVC.svc_kernel': [np.int64(2)],\n",
       "    'mySVC.svc_random_state': [np.int64(2)],\n",
       "    'mySVC.svc_shrinking': [np.int64(2)],\n",
       "    'mySVC.svc_tol': [np.int64(2)],\n",
       "    'preprocessing': [np.int64(2)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [np.int64(2)],\n",
       "    'preprocessing.standard_scaler.with_std': [np.int64(2)]},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(0.8193660529240634)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.4118912728864692)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(0)],\n",
       "    'mySVC.svc_gamma': [np.int64(0)],\n",
       "    'mySVC.svc_kernel': [np.int64(0)],\n",
       "    'mySVC.svc_random_state': [np.int64(4)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(0.000812724359867307)],\n",
       "    'preprocessing': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [np.int64(1)],\n",
       "    'preprocessing.standard_scaler.with_std': [np.int64(1)]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 23, 132000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 44, 925000)},\n",
       " {'state': 2,\n",
       "  'tid': 3,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.1657559198542805,\n",
       "   'loss_variance': 0.00025233739942982086,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.058792829513549805},\n",
       "  'misc': {'tid': 3,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(3)],\n",
       "    'mySVC.svc_coef0': [np.int64(3)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(3)],\n",
       "    'mySVC.svc_degree': [np.int64(3)],\n",
       "    'mySVC.svc_gamma': [np.int64(3)],\n",
       "    'mySVC.svc_kernel': [np.int64(3)],\n",
       "    'mySVC.svc_random_state': [np.int64(3)],\n",
       "    'mySVC.svc_shrinking': [np.int64(3)],\n",
       "    'mySVC.svc_tol': [np.int64(3)],\n",
       "    'preprocessing': [np.int64(3)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(3)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(3)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(0.9115521691616473)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.9113288342746465)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(1)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(0)],\n",
       "    'mySVC.svc_random_state': [np.int64(1)],\n",
       "    'mySVC.svc_shrinking': [np.int64(1)],\n",
       "    'mySVC.svc_tol': [np.float64(0.0007814612632719293)],\n",
       "    'preprocessing': [np.int64(2)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(0)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(0)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 44, 939000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 7000)},\n",
       " {'state': 2,\n",
       "  'tid': 4,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.16393442622950816,\n",
       "   'loss_variance': 0.0002501093615443615,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.12779521942138672},\n",
       "  'misc': {'tid': 4,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(4)],\n",
       "    'mySVC.svc_coef0': [np.int64(4)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(4)],\n",
       "    'mySVC.svc_degree': [np.int64(4)],\n",
       "    'mySVC.svc_gamma': [np.int64(4)],\n",
       "    'mySVC.svc_kernel': [np.int64(4)],\n",
       "    'mySVC.svc_random_state': [np.int64(4)],\n",
       "    'mySVC.svc_shrinking': [np.int64(4)],\n",
       "    'mySVC.svc_tol': [np.int64(4)],\n",
       "    'preprocessing': [np.int64(4)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [np.int64(4)],\n",
       "    'preprocessing.standard_scaler.with_std': [np.int64(4)]},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(0.8661828616263555)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.32310053227283486)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(4)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(2)],\n",
       "    'mySVC.svc_random_state': [np.int64(3)],\n",
       "    'mySVC.svc_shrinking': [np.int64(1)],\n",
       "    'mySVC.svc_tol': [np.float64(0.0019357643730025579)],\n",
       "    'preprocessing': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [np.int64(1)],\n",
       "    'preprocessing.standard_scaler.with_std': [np.int64(0)]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 19000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 156000)},\n",
       " {'state': 2,\n",
       "  'tid': 5,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.26047358834244083,\n",
       "   'loss_variance': 0.000351509303135864,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.09504461288452148},\n",
       "  'misc': {'tid': 5,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(5)],\n",
       "    'mySVC.svc_coef0': [np.int64(5)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(5)],\n",
       "    'mySVC.svc_degree': [np.int64(5)],\n",
       "    'mySVC.svc_gamma': [np.int64(5)],\n",
       "    'mySVC.svc_kernel': [np.int64(5)],\n",
       "    'mySVC.svc_random_state': [np.int64(5)],\n",
       "    'mySVC.svc_shrinking': [np.int64(5)],\n",
       "    'mySVC.svc_tol': [np.int64(5)],\n",
       "    'preprocessing': [np.int64(5)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(5)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(5)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(1.4160813887147201)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.5693480904234842)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(3)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(3)],\n",
       "    'mySVC.svc_random_state': [np.int64(0)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(0.00010075172592135947)],\n",
       "    'preprocessing': [np.int64(2)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(0)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 166000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 271000)},\n",
       " {'state': 2,\n",
       "  'tid': 6,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.1657559198542805,\n",
       "   'loss_variance': 0.00025233739942982086,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.05449080467224121},\n",
       "  'misc': {'tid': 6,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(6)],\n",
       "    'mySVC.svc_coef0': [np.int64(6)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(6)],\n",
       "    'mySVC.svc_degree': [np.int64(6)],\n",
       "    'mySVC.svc_gamma': [np.int64(6)],\n",
       "    'mySVC.svc_kernel': [np.int64(6)],\n",
       "    'mySVC.svc_random_state': [np.int64(6)],\n",
       "    'mySVC.svc_shrinking': [np.int64(6)],\n",
       "    'mySVC.svc_tol': [np.int64(6)],\n",
       "    'preprocessing': [np.int64(6)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(6)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(6)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(0.7772917282338683)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.8179969542125467)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(0)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(0)],\n",
       "    'mySVC.svc_random_state': [np.int64(1)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(3.6874723969396666e-05)],\n",
       "    'preprocessing': [np.int64(2)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(1)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 281000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 345000)},\n",
       " {'state': 2,\n",
       "  'tid': 7,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.2622950819672131,\n",
       "   'loss_variance': 0.0003530955692390986,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.15530157089233398},\n",
       "  'misc': {'tid': 7,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(7)],\n",
       "    'mySVC.svc_coef0': [np.int64(7)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(7)],\n",
       "    'mySVC.svc_degree': [np.int64(7)],\n",
       "    'mySVC.svc_gamma': [np.int64(7)],\n",
       "    'mySVC.svc_kernel': [np.int64(7)],\n",
       "    'mySVC.svc_random_state': [np.int64(7)],\n",
       "    'mySVC.svc_shrinking': [np.int64(7)],\n",
       "    'mySVC.svc_tol': [np.int64(7)],\n",
       "    'preprocessing': [np.int64(7)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [np.int64(7)],\n",
       "    'preprocessing.pca.whiten': [np.int64(7)],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(0.7387686301816984)],\n",
       "    'mySVC.svc_coef0': [np.float64(9.774540855200797e-05)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(1)],\n",
       "    'mySVC.svc_degree': [np.int64(1)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(3)],\n",
       "    'mySVC.svc_random_state': [np.int64(1)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(3.1512661172498735e-05)],\n",
       "    'preprocessing': [np.int64(0)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [np.float64(25.0)],\n",
       "    'preprocessing.pca.whiten': [np.int64(1)],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 357000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 521000)},\n",
       " {'state': 2,\n",
       "  'tid': 8,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.27686703096539167,\n",
       "   'loss_variance': 0.000365349777609125,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.10634756088256836},\n",
       "  'misc': {'tid': 8,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(8)],\n",
       "    'mySVC.svc_coef0': [np.int64(8)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(8)],\n",
       "    'mySVC.svc_degree': [np.int64(8)],\n",
       "    'mySVC.svc_gamma': [np.int64(8)],\n",
       "    'mySVC.svc_kernel': [np.int64(8)],\n",
       "    'mySVC.svc_random_state': [np.int64(8)],\n",
       "    'mySVC.svc_shrinking': [np.int64(8)],\n",
       "    'mySVC.svc_tol': [np.int64(8)],\n",
       "    'preprocessing': [np.int64(8)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [np.int64(8)],\n",
       "    'preprocessing.pca.whiten': [np.int64(8)],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(1.2240695778932102)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.8748165137647407)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(0)],\n",
       "    'mySVC.svc_degree': [np.int64(3)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(3)],\n",
       "    'mySVC.svc_random_state': [np.int64(1)],\n",
       "    'mySVC.svc_shrinking': [np.int64(0)],\n",
       "    'mySVC.svc_tol': [np.float64(0.0005363772013884835)],\n",
       "    'preprocessing': [np.int64(0)],\n",
       "    'preprocessing.min_max_scaler.clip': [],\n",
       "    'preprocessing.min_max_scaler.feature_min': [],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [np.float64(9.0)],\n",
       "    'preprocessing.pca.whiten': [np.int64(1)],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 531000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 647000)},\n",
       " {'state': 2,\n",
       "  'tid': 9,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 0.22040072859744986,\n",
       "   'loss_variance': 0.00031354789677219547,\n",
       "   'status': 'ok',\n",
       "   'duration': 0.10100126266479492},\n",
       "  'misc': {'tid': 9,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'mySVC.svc_C': [np.int64(9)],\n",
       "    'mySVC.svc_coef0': [np.int64(9)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(9)],\n",
       "    'mySVC.svc_degree': [np.int64(9)],\n",
       "    'mySVC.svc_gamma': [np.int64(9)],\n",
       "    'mySVC.svc_kernel': [np.int64(9)],\n",
       "    'mySVC.svc_random_state': [np.int64(9)],\n",
       "    'mySVC.svc_shrinking': [np.int64(9)],\n",
       "    'mySVC.svc_tol': [np.int64(9)],\n",
       "    'preprocessing': [np.int64(9)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(9)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(9)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []},\n",
       "   'vals': {'mySVC.svc_C': [np.float64(1.0041042395355118)],\n",
       "    'mySVC.svc_coef0': [np.float64(0.42943127507683165)],\n",
       "    'mySVC.svc_decision_function_shape': [np.int64(1)],\n",
       "    'mySVC.svc_degree': [np.int64(2)],\n",
       "    'mySVC.svc_gamma': [np.int64(1)],\n",
       "    'mySVC.svc_kernel': [np.int64(3)],\n",
       "    'mySVC.svc_random_state': [np.int64(0)],\n",
       "    'mySVC.svc_shrinking': [np.int64(1)],\n",
       "    'mySVC.svc_tol': [np.float64(6.353797866813163e-05)],\n",
       "    'preprocessing': [np.int64(2)],\n",
       "    'preprocessing.min_max_scaler.clip': [np.int64(1)],\n",
       "    'preprocessing.min_max_scaler.feature_min': [np.int64(1)],\n",
       "    'preprocessing.normalizer.norm': [],\n",
       "    'preprocessing.pca.n_components': [],\n",
       "    'preprocessing.pca.whiten': [],\n",
       "    'preprocessing.standard_scaler.with_mean': [],\n",
       "    'preprocessing.standard_scaler.with_std': []}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 658000),\n",
       "  'refresh_time': datetime.datetime(2024, 8, 22, 7, 15, 45, 769000)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can investigate the individual trials \n",
    "estim.trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca03a600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.1657559198542805,\n",
       "  'loss_variance': 0.00025233739942982086,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.08312249183654785},\n",
       " {'loss': 0.1657559198542805,\n",
       "  'loss_variance': 0.00025233739942982086,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.14423751831054688},\n",
       " {'loss': 0.1657559198542805,\n",
       "  'loss_variance': 0.00025233739942982086,\n",
       "  'status': 'ok',\n",
       "  'duration': 21.782816886901855},\n",
       " {'loss': 0.1657559198542805,\n",
       "  'loss_variance': 0.00025233739942982086,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.058792829513549805},\n",
       " {'loss': 0.16393442622950816,\n",
       "  'loss_variance': 0.0002501093615443615,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.12779521942138672},\n",
       " {'loss': 0.26047358834244083,\n",
       "  'loss_variance': 0.000351509303135864,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.09504461288452148},\n",
       " {'loss': 0.1657559198542805,\n",
       "  'loss_variance': 0.00025233739942982086,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.05449080467224121},\n",
       " {'loss': 0.2622950819672131,\n",
       "  'loss_variance': 0.0003530955692390986,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.15530157089233398},\n",
       " {'loss': 0.27686703096539167,\n",
       "  'loss_variance': 0.000365349777609125,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.10634756088256836},\n",
       " {'loss': 0.22040072859744986,\n",
       "  'loss_variance': 0.00031354789677219547,\n",
       "  'status': 'ok',\n",
       "  'duration': 0.10100126266479492}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trials scores\n",
    "estim.trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0920c6bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mySVC.svc_C': [np.float64(1.0934211895658286),\n",
       "  np.float64(1.2775781542665854),\n",
       "  np.float64(0.8193660529240634),\n",
       "  np.float64(0.9115521691616473),\n",
       "  np.float64(0.8661828616263555),\n",
       "  np.float64(1.4160813887147201),\n",
       "  np.float64(0.7772917282338683),\n",
       "  np.float64(0.7387686301816984),\n",
       "  np.float64(1.2240695778932102),\n",
       "  np.float64(1.0041042395355118)],\n",
       " 'mySVC.svc_coef0': [np.float64(0.792540555539598),\n",
       "  np.float64(0.7456018216312651),\n",
       "  np.float64(0.4118912728864692),\n",
       "  np.float64(0.9113288342746465),\n",
       "  np.float64(0.32310053227283486),\n",
       "  np.float64(0.5693480904234842),\n",
       "  np.float64(0.8179969542125467),\n",
       "  np.float64(9.774540855200797e-05),\n",
       "  np.float64(0.8748165137647407),\n",
       "  np.float64(0.42943127507683165)],\n",
       " 'mySVC.svc_decision_function_shape': [np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(1)],\n",
       " 'mySVC.svc_degree': [np.int64(2),\n",
       "  np.int64(4),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(4),\n",
       "  np.int64(3),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(3),\n",
       "  np.int64(2)],\n",
       " 'mySVC.svc_gamma': [np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1)],\n",
       " 'mySVC.svc_kernel': [np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(2),\n",
       "  np.int64(3),\n",
       "  np.int64(0),\n",
       "  np.int64(3),\n",
       "  np.int64(3),\n",
       "  np.int64(3)],\n",
       " 'mySVC.svc_random_state': [np.int64(3),\n",
       "  np.int64(2),\n",
       "  np.int64(4),\n",
       "  np.int64(1),\n",
       "  np.int64(3),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(0)],\n",
       " 'mySVC.svc_shrinking': [np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1)],\n",
       " 'mySVC.svc_tol': [np.float64(0.002969556984771239),\n",
       "  np.float64(1.249411875400459e-05),\n",
       "  np.float64(0.000812724359867307),\n",
       "  np.float64(0.0007814612632719293),\n",
       "  np.float64(0.0019357643730025579),\n",
       "  np.float64(0.00010075172592135947),\n",
       "  np.float64(3.6874723969396666e-05),\n",
       "  np.float64(3.1512661172498735e-05),\n",
       "  np.float64(0.0005363772013884835),\n",
       "  np.float64(6.353797866813163e-05)],\n",
       " 'preprocessing': [np.int64(3),\n",
       "  np.int64(3),\n",
       "  np.int64(1),\n",
       "  np.int64(2),\n",
       "  np.int64(1),\n",
       "  np.int64(2),\n",
       "  np.int64(2),\n",
       "  np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(2)],\n",
       " 'preprocessing.min_max_scaler.clip': [np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1),\n",
       "  np.int64(1)],\n",
       " 'preprocessing.min_max_scaler.feature_min': [np.int64(0),\n",
       "  np.int64(0),\n",
       "  np.int64(1),\n",
       "  np.int64(1)],\n",
       " 'preprocessing.normalizer.norm': [np.int64(2), np.int64(2)],\n",
       " 'preprocessing.pca.n_components': [np.float64(25.0), np.float64(9.0)],\n",
       " 'preprocessing.pca.whiten': [np.int64(1), np.int64(1)],\n",
       " 'preprocessing.standard_scaler.with_mean': [np.int64(1), np.int64(1)],\n",
       " 'preprocessing.standard_scaler.with_std': [np.int64(1), np.int64(0)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trials tested values:\n",
    "estim.trials.vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafaf0e",
   "metadata": {},
   "source": [
    "As far as one can tell, the categorical hyper-parameters indexes corresponds to the order in which they are cited in the sklearn-doc.\n",
    "\n",
    "For example, for the SVC kernel if we look at the [sklearn SVC doc](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) we get `{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}`.\n",
    "\n",
    "So in the trials:\n",
    "\n",
    "```\n",
    "mySVC.svc_kernel': [np.int64(1),\n",
    "  np.int64(1),\n",
    "  np.int64(0),\n",
    "  np.int64(0),\n",
    "  np.int64(2),\n",
    "...\n",
    "```\n",
    "\n",
    "Corresponds to successive trials of  of poly, poly, linear, linear, and then rbf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d0f0d",
   "metadata": {},
   "source": [
    "## adding a preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f38dec",
   "metadata": {},
   "source": [
    " * preprocessing steps are given in a list (so you can have multiple successive preprocessing steps)\n",
    " * use an empty list `[]` f you don't want any pre-processing\n",
    " * you can fix the value of any hyper-parameter by giving it as an argument. Any hyperparameter of the original sklearn object is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee57e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function standard_scaler in module hpsklearn.components.preprocessing._data:\n",
      "\n",
      "standard_scaler(name: str, copy: bool = True, with_mean: Union[bool, hyperopt.pyll.base.Apply] = None, with_std: Union[bool, hyperopt.pyll.base.Apply] = None)\n",
      "    Return a pyll graph with hyperparameters that will construct\n",
      "    a sklearn.preprocessing.StandardScaler transformer.\n",
      "    \n",
      "    Args:\n",
      "         name: name | str\n",
      "         copy: perform inplace scaling or on copy | bool\n",
      "         with_mean: center data before scaling | bool\n",
      "         with_std: scale data to unit variance | bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## help of the hpsklearn wrapper\n",
    "help( standard_scaler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c3f6eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance.\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  than others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  `StandardScaler` is sensitive to outliers, and the features may scale\n",
      " |  differently from each other in the presence of outliers. For an example\n",
      " |  visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
      " |  <plot_all_scaling_standard_scaler_section>`.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : bool, default=True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : bool, default=True\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : bool, default=True\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray of shape (n_features,) or None\n",
      " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
      " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      " |      variance is zero, we can't achieve unit variance, and the data is left\n",
      " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      " |      when `with_std=False`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,) or None\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_features,) or None\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
      " |      ``with_std=False``.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array of dtype int. If\n",
      " |      `sample_weights` are used it will be a float (if no missing data)\n",
      " |      or an array of dtype float that sums the weights seen so far.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  scale : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      " |      correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.OneToOneFeatureMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils._set_output._SetOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.24\n",
      " |             parameter *sample_weight* support to StandardScaler.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_inverse_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``inverse_transform`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``copy`` parameter in ``inverse_transform``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_partial_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``partial_fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``transform`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``copy`` parameter in ``transform``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, default=None\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |  \n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Input features.\n",
      " |      \n",
      " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      " |            used as feature names in. If `feature_names_in_` is not defined,\n",
      " |            then the following input feature names are generated:\n",
      " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      " |          - If `input_features` is an array-like, then `input_features` must\n",
      " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Same as input features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  set_output(self, *, transform=None)\n",
      " |      Set output container.\n",
      " |      \n",
      " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      " |      for an example on how to use the API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      " |          Configure output of `transform` and `fit_transform`.\n",
      " |      \n",
      " |          - `\"default\"`: Default output format of a transformer\n",
      " |          - `\"pandas\"`: DataFrame output\n",
      " |          - `\"polars\"`: Polars output\n",
      " |          - `None`: Transform configuration is unchanged\n",
      " |      \n",
      " |          .. versionadded:: 1.4\n",
      " |              `\"polars\"` option was added.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      " |  \n",
      " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "## help of the sklearn StandardScaler\n",
    "help( StandardScaler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "889d72ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.18trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.52trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.03trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 13.27trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  4.34trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  6.12trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  8.40trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  3.81trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  2.66trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00,  7.90trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00,  8.49trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00,  7.07trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 10.27trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00,  5.73trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00,  5.14trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 14.45trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00,  6.57trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00,  5.10trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00,  7.47trial/s, best loss: 0.1657559198542805]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00,  6.13trial/s, best loss: 0.1657559198542805]\n",
      "0.8491803278688524\n",
      "CPU times: user 418 ms, sys: 119 ms, total: 537 ms\n",
      "Wall time: 3.63 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', with_mean=True,with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\"),\n",
    "                          max_evals=20, ## increasing the number of trials\n",
    "                          trial_timeout=120)\n",
    "    \n",
    "estim.fit(X_train, y_train)\n",
    "\n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287acbfc",
   "metadata": {},
   "source": [
    "Experience may vary, but fixing the preprocessing sped up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cb90e",
   "metadata": {},
   "source": [
    "## changing the cross-validation scheme\n",
    "\n",
    "Remeber, the default is 80% train, 20% validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6809de1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/__init__.py'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpsklearn.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c90b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module hpsklearn.estimator.estimator:\n",
      "\n",
      "fit(self, X, y, EX_list: Union[list, tuple] = None, valid_size: float = 0.2, n_folds: int = None, kfolds_group: Union[list, numpy.ndarray] = None, cv_shuffle: bool = False, warm_start: bool = False, random_state: numpy.random._generator.Generator = Generator(PCG64) at 0x7FA8267D4900) -> None\n",
      "    Search the space of learners and preprocessing steps for a good\n",
      "    predictive model of y <- X. Store the best model for predictions.\n",
      "    \n",
      "    Args:\n",
      "        X:\n",
      "            Input variables\n",
      "    \n",
      "        y:\n",
      "            Output variables\n",
      "    \n",
      "        EX_list: list, default is None\n",
      "            List of exogenous datasets. Each must have the same number of\n",
      "            samples as X.\n",
      "    \n",
      "        valid_size: float, default is 0.2\n",
      "            The portion of the dataset used as the validation set. If\n",
      "            cv_shuffle is False, always use the last samples as validation.\n",
      "    \n",
      "        n_folds: int, default is None\n",
      "            When n_folds is not None, use K-fold cross-validation when\n",
      "            n_folds > 2. Or, use leave-one-out cross-validation when\n",
      "            n_folds = -1. For Group K-fold cross-validation, functions as\n",
      "            `n_splits`.\n",
      "    \n",
      "        kfolds_group: list or ndarray, default is None\n",
      "            When kfolds_group is not None, use Group K-fold cross-validation\n",
      "            with the specified groups. The length of group_kfolds must be\n",
      "            equal to the number of samples in X.\n",
      "    \n",
      "        cv_shuffle: bool, default is False\n",
      "            Whether to perform sample shuffling before splitting the\n",
      "            data into training and validation sets.\n",
      "    \n",
      "        warm_start: bool, default is False\n",
      "            If True, the estimator will start from an existing sequence\n",
      "            of trials.\n",
      "    \n",
      "        random_state: Generator, default is np.random.default_rng()\n",
      "            The random state used to seed the cross-validation shuffling.\n",
      "    \n",
      "    Notes:\n",
      "        For classification problems, hpsklearn will always use the stratified\n",
      "        version of the K-fold cross-validation or shuffle-and-split.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HyperoptEstimator.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22a648ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.46trial/s, best loss: 0.22858184469558873]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.30trial/s, best loss: 0.15238789646372586]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:00<00:00, 120.11s/trial, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  2.52trial/s, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  2.81trial/s, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  3.71trial/s, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  3.21trial/s, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  3.22trial/s, best loss: 0.15238789646372586]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  2.33trial/s, best loss: 0.15238789646372586]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00,  3.59trial/s, best loss: 0.15238789646372586]\n",
      "0.8469945355191257\n",
      "CPU times: user 390 ms, sys: 49.8 ms, total: 440 ms\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', with_mean=True,with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\"),\n",
    "                          trial_timeout=120)\n",
    "    \n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)\n",
    "    \n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b82fd3",
   "metadata": {},
   "source": [
    "## changing the score to optimize\n",
    "\n",
    "The default is accuracy, but we know it is far from ideal, in particular when there is imbalance\n",
    "\n",
    "\n",
    "> in a regression problem, the default score is $R^2$\n",
    "\n",
    "What we need is to give to `HyperoptEstimator` a function that takes:\n",
    "\n",
    " * true target values\n",
    " * predited target values\n",
    " \n",
    "An returns a score that need to be **minimized** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df2a6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# balanced accuracy is nice, \n",
    "# we just have to adapt it so that it can be minimized \n",
    "# -1 * balanced_accuracy will work:\n",
    "\n",
    "balanced_accuracy_loss = lambda y_target, y_prediction : -balanced_accuracy_score(y_target, y_prediction)\n",
    "\n",
    "def balanced_accuracy_loss( y_target, y_prediction) : \n",
    "    print( y_target.shape , y_prediction.shape ) ## looking up shapes\n",
    "    return -balanced_accuracy_score(y_target, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a72fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.53trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  1.32trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  2.07trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  2.46trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  2.07trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  2.15trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  2.57trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  2.02trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  2.33trial/s, best loss: -0.6719977362761743]\n",
      "(2743,)                                                                                                                                                                                  \n",
      "(2743,)                                                                                                                                                                                  \n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00,  2.07trial/s, best loss: -0.6719977362761743]\n",
      "0.653551912568306\n",
      "CPU times: user 406 ms, sys: 95.4 ms, total: 502 ms\n",
      "Wall time: 5.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', \n",
    "                                                          with_mean=True,\n",
    "                                                          with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\",\n",
    "                                         class_weight='balanced'), ## fixing  balanced class weigth scheme\n",
    "                          loss_fn = balanced_accuracy_loss,## we give our custom loss function\n",
    "                          trial_timeout=120)\n",
    "    \n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)\n",
    "    \n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21a995b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy on test 0.6628717644441149\n",
      "         accuracy on test 0.653551912568306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"balanced accuracy on test\" , balanced_accuracy_score( y_test , estim.predict(X_test) ) )\n",
    "print(\"         accuracy on test\" , accuracy_score( y_test , estim.predict(X_test) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d886b3",
   "metadata": {},
   "source": [
    "Note that `estim.score()` still gives you the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f10b5",
   "metadata": {},
   "source": [
    "If instead of the balanced accuracy, we go for a score that needs the predicted probabilities (or score),\n",
    "in theory we need to:\n",
    "\n",
    " 1. set `continuous_loss_fn = True` in `HyperoptEstimator` -> predictions will be made with `.predict_proba()` instead of `.predict()`\n",
    " 2. that the tested classifier has a `.predict_proba()` method\n",
    " \n",
    "BUT, it practice it gets more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a54ea878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def roc_auc_loss(y_target, y_prediction ):\n",
    "    return -roc_auc_score(y_target, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9926f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Found input variables with inconsistent numbers of samples: [2743, 5486]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/1 [00:01<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2743, 5486]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:480\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     increment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment,\n\u001b[1;32m    479\u001b[0m                     adjusted_max_evals \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[0;32m--> 480\u001b[0m     fit_iter\u001b[38;5;241m.\u001b[39msend(increment)\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment_dump_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment_dump_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dump_file:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:347\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Workaround for rstate issue #35\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrstate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetfullargspec(hyperopt\u001b[38;5;241m.\u001b[39mfmin)\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m--> 347\u001b[0m     \u001b[43mhyperopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fn_with_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                  \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# -- let exceptions crash the program, so we notice them.\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# -- in case no success so far\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:319\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit_iter.<locals>._fn_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fn_rval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_rval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fn_rval[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# -- remove potentially large objects from the rval\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#    so that the Trials() object below stays small\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m#    We can recompute them if necessary, and it's usually\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#    not necessary at all.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_rval[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m hyperopt\u001b[38;5;241m.\u001b[39mSTATUS_OK:\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2743, 5486]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', with_mean=True,with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\",probability=True),\n",
    "                          loss_fn = roc_auc_loss,\n",
    "                          continuous_loss_fn = True,\n",
    "                          trial_timeout=10)\n",
    "    \n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)\n",
    "    \n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b714b",
   "metadata": {},
   "source": [
    "We get a error: `job exception: Found input variables with inconsistent numbers of samples: [2743, 5486]`\n",
    "\n",
    "It stems from out loss function; let's investigate further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4425462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2743,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## incompatible dimensions: [2743, 5486]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1a7b6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5486"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2743*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ce650",
   "metadata": {},
   "source": [
    "**Question:** What do you think happened?\n",
    "\n",
    "---\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "scroll below for answer:\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7a2a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85130893, 0.14869107],\n",
       "       [0.85236616, 0.14763384],\n",
       "       [0.85213178, 0.14786822],\n",
       "       ...,\n",
       "       [0.85138043, 0.14861957],\n",
       "       [0.85226136, 0.14773864],\n",
       "       [0.8526272 , 0.1473728 ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## have a look at the predict_proba() output from the sklearn_object:\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "sklearn_svc = SVC(probability=True)\n",
    "sklearn_svc.fit(X_train,y_train)\n",
    "sklearn_svc.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a38970",
   "metadata": {},
   "source": [
    "`predict_proba()` returns one column per category, and our loss function got twice the expected amount of values : `hpsklearn` has flattened the whole output before sending it to the function.\n",
    "\n",
    "Ideally, we would have the second columns (probability of being of the positive category).\n",
    "\n",
    "The flattening worked in a way where these elements correspond to one every two elements now.\n",
    "\n",
    "We can check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "390fbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2743,)                                                                                                                                                                                  \n",
      "(5486,)                                                                                                                                                                                  \n",
      "[0.84783455 0.15216545 0.84783541 0.15216459 0.84783465 0.15216535                                                                                                                       \n",
      " 0.84785148 0.15214852 0.84783282 0.15216718]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                                                                                                                                                          \n",
      "  0%|                                                                                                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Found input variables with inconsistent numbers of samples: [2743, 5486]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2743, 5486]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mroc_auc_score(y_target, y_prediction)\n\u001b[1;32m     10\u001b[0m estim \u001b[38;5;241m=\u001b[39m HyperoptEstimator(preprocessing\u001b[38;5;241m=\u001b[39m[ standard_scaler(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssc\u001b[39m\u001b[38;5;124m'\u001b[39m, with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,with_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) ],\n\u001b[1;32m     11\u001b[0m                           classifier\u001b[38;5;241m=\u001b[39msvc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmySVC\u001b[39m\u001b[38;5;124m\"\u001b[39m,probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     12\u001b[0m                           loss_fn \u001b[38;5;241m=\u001b[39m roc_auc_loss,\n\u001b[1;32m     13\u001b[0m                           continuous_loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                           trial_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mestim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_shuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:480\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     increment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment,\n\u001b[1;32m    479\u001b[0m                     adjusted_max_evals \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[0;32m--> 480\u001b[0m     fit_iter\u001b[38;5;241m.\u001b[39msend(increment)\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment_dump_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_increment_dump_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dump_file:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:347\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Workaround for rstate issue #35\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrstate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetfullargspec(hyperopt\u001b[38;5;241m.\u001b[39mfmin)\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m--> 347\u001b[0m     \u001b[43mhyperopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fn_with_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                  \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# -- let exceptions crash the program, so we notice them.\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# -- in case no success so far\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/hpsklearn/estimator/estimator.py:319\u001b[0m, in \u001b[0;36mhyperopt_estimator.fit_iter.<locals>._fn_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fn_rval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_rval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fn_rval[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# -- remove potentially large objects from the rval\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m#    so that the Trials() object below stays small\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m#    We can recompute them if necessary, and it's usually\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m#    not necessary at all.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn_rval[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m hyperopt\u001b[38;5;241m.\u001b[39mSTATUS_OK:\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2743, 5486]"
     ]
    }
   ],
   "source": [
    "def roc_auc_loss(y_target, y_prediction ):\n",
    "    print( y_target.shape , y_prediction.shape ) ## looking up shapes\n",
    "    print( y_prediction[:10] ) ## looking up what's in the predictions\n",
    "    p0 = y_prediction[::2] ## half of the elements\n",
    "    p1 = y_prediction[1::2] ## other half of the elements\n",
    "    print( (p0+p1)[:10] )  ## what do they sum to?\n",
    "    \n",
    "    return -roc_auc_score(y_target, y_prediction)\n",
    "\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', with_mean=True,with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\",probability=True),\n",
    "                          loss_fn = roc_auc_loss,\n",
    "                          continuous_loss_fn = True,\n",
    "                          trial_timeout=10)\n",
    "    \n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31041578",
   "metadata": {},
   "source": [
    "the probabilities of being of category 0 and category 1 sum to 1.0 --> we have the correct formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e49f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_loss_fixed( y_target, y_prediction ):\n",
    "    p1 = y_prediction[1::2] ## half of the elements corresponding to proba of being category 1    \n",
    "    return -roc_auc_score(y_target, p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a319a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.09s/trial, best loss: -0.5224417348356227]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.83s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.10s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  1.46trial/s, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  1.04s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  1.16s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  1.16s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  1.14s/trial, best loss: -0.611014045377373]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  1.85trial/s, best loss: -0.611014045377373]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  1.71s/trial, best loss: -0.611014045377373]\n",
      "CPU times: user 1.59 s, sys: 66.6 ms, total: 1.66 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', with_mean=True,with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\",probability=True),\n",
    "                          loss_fn = roc_auc_loss_fixed,\n",
    "                          continuous_loss_fn = True,\n",
    "                          trial_timeout=10)\n",
    "    \n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808d737",
   "metadata": {},
   "source": [
    "\n",
    "And now it works. The issue has been reported a number of time in the library github page.\n",
    "\n",
    "Perhaps by the time you go through this notebook this has been solved, but in the meantime you can use our quick fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10300ff4",
   "metadata": {},
   "source": [
    "## warm start : continue searching for better solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84cf1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00,  1.01trial/s, best loss: -0.611014045377373]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00,  1.32s/trial, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00,  2.01trial/s, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00,  1.35s/trial, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:01<00:00,  1.03s/trial, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00,  1.48trial/s, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:00<00:00,  1.00trial/s, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00,  1.94trial/s, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00,  1.08s/trial, best loss: -0.6119493749035345]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00,  1.47trial/s, best loss: -0.6119493749035345]\n"
     ]
    }
   ],
   "source": [
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True , warm_start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce68913",
   "metadata": {},
   "source": [
    "## specifying your own research space \n",
    "\n",
    "The default search space are OK, but you have the possibility of changing which hyper-parameter you tune, and how you explore them, maybe to point the algorithm in a direction you know is more likely to yield good results.\n",
    "\n",
    "The way it works is that for each hyper-parameter you define a **prior distribution** (it is bayesian afterall) using one of hyperopt function.\n",
    "\n",
    "For a more detailed documentation we refer you to the [hyperopt documentation](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d5c89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import numpy as np\n",
    "\n",
    "## kernel chosen between rbf, poly and linear, with probability of 0.5,0.25 and 0.25 resp.\n",
    "svc_kernel = hp.pchoice(\"kernel\", [(0.50, \"rbf\"), \n",
    "                                   (0.25, \"poly\"), \n",
    "                                   (0.25, \"linear\")])\n",
    "\n",
    "## choosing C uniformly in the log space between 10**-5 and 10**1\n",
    "svc_C = hp.loguniform(\"C\", low=np.log(1e-5), high=np.log(10))\n",
    "\n",
    "## choosing coef0 using a normal distribution with mean 0 and std dev 1\n",
    "svc_coef0 = hp.normal(\"coef0\", mu=0 , sigma=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6c7ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.60trial/s, best loss: -0.5930534547512476]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  1.26trial/s, best loss: -0.5930534547512476]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  2.06trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  2.26trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  1.06trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  1.93trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  2.09trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00,  1.79trial/s, best loss: -0.6445938159180943]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  1.58trial/s, best loss: -0.6445938159180943]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00,  1.99trial/s, best loss: -0.6445938159180943]\n",
      "0.7169398907103826\n",
      "CPU times: user 457 ms, sys: 72.9 ms, total: 530 ms\n",
      "Wall time: 6.42 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', \n",
    "                                                          with_mean=True,\n",
    "                                                          with_std=True) ],\n",
    "                          classifier=svc(\"mySVC\",\n",
    "                                         probability=True,\n",
    "                                         kernel =svc_kernel,\n",
    "                                         C =svc_C,\n",
    "                                         coef0 =svc_coef0),\n",
    "                          loss_fn = roc_auc_loss_fixed,\n",
    "                          continuous_loss_fn = True,\n",
    "                          trial_timeout=10)\n",
    "\n",
    "estim.fit(X_train, y_train , n_folds=3, cv_shuffle = True)\n",
    "    \n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847bf01",
   "metadata": {},
   "source": [
    "It is possible nest `hp.choice()` or `hp.pchoice()` to specify more complex search spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20f80614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.47trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.68trial/s, best loss: 0.1657559198542805]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 31.65trial/s, best loss: 0.1657559198542805]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 35.09trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.43trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 37.76trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00,  8.42trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 25.00trial/s, best loss: 0.16393442622950816]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  8.64trial/s, best loss: 0.16393442622950816]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.53trial/s, best loss: 0.16393442622950816]\n"
     ]
    }
   ],
   "source": [
    "from hpsklearn import decision_tree_classifier \n",
    "\n",
    "\n",
    "space = hp.choice('classifier', [  decision_tree_classifier('myTree') , \n",
    "                                   svc('mySVC' , \n",
    "                                       kernel =svc_kernel,\n",
    "                                       C =svc_C,\n",
    "                                       coef0 =svc_coef0 ) ] )\n",
    "\n",
    "estim = HyperoptEstimator(preprocessing=[ standard_scaler('ssc', \n",
    "                                                          with_mean=True,\n",
    "                                                          with_std=True) ],\n",
    "                          classifier=space,\n",
    "                          trial_timeout=10)\n",
    "\n",
    "estim.fit(X_train, y_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee4a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f0532f",
   "metadata": {},
   "source": [
    "## exercise \n",
    "\n",
    "setup tuning with HyperoptEstimator for an XGBoost model\n",
    "\n",
    " * which hyper-parameters are tuned?\n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5e1a420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpsklearn import xgboost_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a75259a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:41] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:44] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/trial, best loss: -0.7093327159541081]\n",
      " 50%|███████████████████████████████████████████████████████████████████████                                                                       | 1/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:44] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:45] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.98s/trial, best loss: -0.7106672840458919]\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                               | 2/3 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:47] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  2.12s/trial, best loss: -0.7106672840458919]\n",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                   | 3/4 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:48] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:49] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:50] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:51] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  3.83s/trial, best loss: -0.7106672840458919]\n",
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 4/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:52] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:02:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:00] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:03] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00, 13.22s/trial, best loss: -0.7106672840458919]\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 5/6 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:05] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:06] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:04<00:00,  4.98s/trial, best loss: -0.7106672840458919]\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 6/7 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:16] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  8.74s/trial, best loss: -0.7106672840458919]\n",
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                 | 7/8 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:20] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  2.03s/trial, best loss: -0.7106672840458919]\n",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 8/9 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:22] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  1.59s/trial, best loss: -0.7106672840458919]\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 9/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:23] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:24] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n",
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  2.32s/trial, best loss: -0.7106672840458919]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/intermediateML/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [13:03:25] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1722454208864/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491803278688524\n",
      "CPU times: user 715 ms, sys: 78.1 ms, total: 793 ms\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estim = HyperoptEstimator(preprocessing=[],\n",
    "                          classifier=xgboost_classification('xgb'),\n",
    "                          loss_fn = roc_auc_loss_fixed,\n",
    "                          continuous_loss_fn = True,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "estim.fit(X_train, y_train , n_folds=5, cv_shuffle = True)\n",
    "    \n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e612bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estim.best_model()['learner']\n",
    "roc_auc_score( y_test , model.predict_proba(X_test)[:,1] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estim.best_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_intermediateml)",
   "language": "python",
   "name": "conda_intermediateml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
